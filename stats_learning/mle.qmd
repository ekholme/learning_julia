---
title: "MLE Learning Out Loud"
description: |
    Learning maximum likelihood estimation by fitting models by hand (in Julia!)
author: "EE"
date: "7/27/2022"
format:
    html:
        code-fold: true
jupyter: julia-1.7
---

*Disclaimer! The whole point of these "learning out loud" blog posts is to give myself a venue in which to practice/learn various statistics and programming concepts. I'm deciding to post these on my website both to normalize this notion of learning in public and also to invite people who know more than me to provide feedback. If I get something wrong, I'd love for you to tell me!*

# Maury Povich as a metaphor for maximum likelihood estimation

So this obviously isn't 100% mathematically rigorous, but based on my (limited) understanding of maximum likelihood estimation (MLE), I think it's kind of like the Maury Povich show...

Back when I was in high school, some of my friends and I used to eat lunch in our track coach's classroom and watch the Maury Povich show...**RESUME HERE**


Let's do some "learning out loud" with maximum likelihood estimation (MLE)

```{julia}
using Distributions
using CairoMakie
using Random
using Optim
using GLM
using DataFrames
```

# Case 1: Fitting a Normal Distribution

This is the simplest case. First, we're going to generate some sample data from a normal distribution with \mu = 0 and \sigma = 1

```{julia}
Random.seed!(0408)
s = rand(Normal(), 10000)

```

Then we'll generate a bunch of normal distributions with various means and standard deviations

```{julia}
Î¼s = collect(-2.0:2.0)
Ïƒs = [0.5:0.5:2;]

ds = []

for i = Î¼s, j = Ïƒs
    d = Normal(i, j)
    push!(ds, d)
end

```

So our task now is going to be to determine the likelihood of each distribution (defined with a given set a parameters) given our data, *s*, that we've drawn from a normal distribution with \mu = 0 and @s = 1

To do this, we use the probability density function of our normal distribution to determine the likelihood of the parameters for any given observation. Fortunately, Julia (and other languages) have tools that can help us do this without having to write out the entire equation by hand. I'll start with the "manual" version, though, just to make sure I get it before moving on

*add here actual equation*

Let's take a look at the first observation and the first distribution we defined:

The first value in our sample is:
```{julia}
s[1]
```

And the first distribution we'll look at is
```{julia}
ds[1]
```

And if we look at the pdf of this, we get:
```{julia}
pdf(ds[1], s[1])
```

I'm not a statistician (hence these learning posts), but my understanding of this is that it generally represents the "fit" of the distribution (and its parameters) to the given sample/data point. These values will be bound between 0 and 1, since they're likelihoods.

The next step is to convert this to a log scale, since logging allows us to sum things rather than multiply them (which we're gonna do soon).

```{julia}
logpdf(ds[1], s[1])
#same as log(pdf(ds[1], s[]1))
```

So this gives us the log likelihood of a given data point. But now we need to do this for all of the data points in our sample to determine the "fit" of the distribution to our whole sample

```{julia}
sum(logpdf.(ds[1], s))
```

The above is the same as:

```{julia}
loglikelihood(ds[1], s)
```

So this gives us the likelihood of a distribution (normal, in this case, defined by parameters \mu and \sigma) given our sample. The goal then is to pick the *best* distribution, which we can do by *maximizing the likelihood*.

Or, apparently, a lot of people minimize the negative loglikelihood, which is the same thing (and called logloss, I guess).

So let's do this for all of the distributions we specified earlier

```{julia}
lls = []

for i in ds
    res = -loglikelihood(i, s)
    push!(lls, res)
end

lls = Float64.(lls)
```


And then we can plot this

```{julia}
ind = collect(1.0:length(ds))

lines(ind, lls)
```

Notice that our negative log likelihood is minimized in the 10th distribution, so let's take a look at what that is

```{julia}
ds[10]
```

This makes sense! This was the distribution that we drew our samples from!

If we want to do this without looking at a plot, we can apparently do this:

```{julia}
#get the index of the minimum value in lls
min_ll = findall(lls .== minimum(lls))

#get the distribution at this index
ds[min_ll]

```

So this tells us that -- of the distributions we tested! -- the most likely distribution given our data is a normal distribution with mean of 0 and standard deviation of 1. This doesn't necessarily mean that this \mu = 0 and \sigma = 1 are the *optimal* parameters. There could be better parameters that we didn't test, and so in the future we'd want to probably use some sort of optimizing functions that can do all of the math for us.

# Case 2: Simple Linear Regression

So now let's move on a bit and try a simple linear regression.

```{julia}
x = collect(0:.1:10)

Ïµ = rand(Normal(0, 1), length(x))

f(x) = 0.5 + 2*x

y = f.(x) .+ Ïµ
```

And then we can plot this

```{julia}
scatter(x, y)
```

Another way to think about the above is that we expect a linear relationship between x and y in the form of

y = \alpha + \beta*x + \epsilon

We need to estimate alpha and beta in a way that optimally fits the line above, and we do this with maximum likelihood

```{julia}
function max_ll_reg(x, y, params)

    Î± = params[1]
    Î² = params[2]
    Ïƒ = params[3]

    yÌ‚ = Î± .+ x.*Î²

    resids = y .- yÌ‚

    d = Normal(0, Ïƒ)

    ll = -loglikelihood(d, resids)
    
    return ll

end
```

And let's see how this works

```{julia}
yy = max_ll_reg(x, y, .5, 2, 1)
```

The next step then is to optimize this.

```{julia}
res = optimize(params -> max_ll_reg(x, y, params), [0.0, 1.0, 1.0])
```

And then this will give us the maximum likelihood solution for our regression equation

```{julia}
Optim.minimizer(res)
```

We can check this by fitting the model with the `GLM` package

```{julia}
data = DataFrame(X = x, Y = y)

ols_res = lm(@formula(Y ~ X), data)
```

et voila, we get the same \alpha and \beta

Note that Julia also lets us solve the equation via the `\` operator:

```{julia}
#we have to include a column of 1s in the matrix to get the intercept
xmat = hcat(ones(length(x)), x)

xmat \ y
```

# Case 3: Multiple Regression

And I think we can extend the same logic above to multiple regression

```{julia}
tmp = randn(100, 3)

ğ— = hcat(ones(100), tmp)

ğš© = [.5, 1, 2, 3]

fâ‚‚(X) = X*ğš©

Ïµ = rand(Normal(0, .5), size(ğ—)[1])

ğ˜ = fâ‚‚(ğ—) + Ïµ
```


```{julia}
function max_ll_mreg(x, y, params)
    ğš© = params[begin:end-1]
    Ïƒ = params[end]

    yÌ‚ = x*ğš©

    resids = y .- yÌ‚

    d = Normal(0, Ïƒ)

    ll = -loglikelihood(d, resids)

    return ll
end
```

```{julia}
start_params = [.4, .5, 1.5, 4.0, 1.0]

mreg_res = optimize(params -> max_ll_mreg(ğ—, ğ˜, params), start_params)
```

```{julia}
Optim.minimizer(mreg_res)
```

```{julia}
ğ— \ ğ˜
```