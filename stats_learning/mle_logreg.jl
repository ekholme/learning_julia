#an extension of my multiple regression learning, but with logistic regression

using GLM
using Distributions
using Random
using Optim
using DataFrames
using CairoMakie
using StatsFuns

Random.seed!(0408)

#get X
tmp = randn(100, 3)
ğ— = hcat(ones(100), tmp)

#get some seed betas to create data
Î² = rand(Uniform(0, .5), 4)

#write a logistic function
#same as logistic.(ğ—*Î²)
fâ‚(x, b) = exp.(x*b) ./ (1 .+ exp.(x*b))

#generate y values
y = Integer.(round.(fâ‚(ğ—, Î²)))

#define function
function ml_logreg(x, y, Î²)

    yÌ‚ = logistic.(x * Î²)

    res = Float64[]

    for i in 1:lastindex(yÌ‚)
        push!(res, logpdf.(Bernoulli(yÌ‚[i]), y[i]))
    end

    ret = -sum(res)

    #the above is the same as this:
    #ll = sum(-1 .* (log.(yÌ‚) .* y .+ (log.(1 .- yÌ‚) .* (1 .- y))))
    return ret
end

#this will return the log-likelihood for the starting values of Î²
test_run = ml_logreg(ğ—, y, Î²)

#optimize beta
res = optimize(params -> ml_logreg(ğ—, y, params), Î²)

#extract the best beta values
Optim.minimizer(res)

#and if we check our work
logreg_res = glm(ğ—, y, Binomial())
#huzzah