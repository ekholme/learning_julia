---
title: "MLE Learning Out Loud 2: Logistic Regression"
description: |
    Learning maximum likelihood estimation by fitting logistic regression by hand
date: "2022-09-23"
format:
    html:
        code-fold: false
jupyter: julia-1.7
---

In a previous post (INSERT LINK), I did some "learning out loud" by practicing estimating a few models via maximum likelihood by hand. In this short blog, I figured I could extend this learning by applying what I learned previously to logistic regression.

As a reminder, the point of these "learning out loud" posts is to give myself a medium to work through concepts. Hopefully these metacognitive exercises will benefits others, too. The concepts I'm covering here are things that I'm either learning anew or brushing back up on after not using for a while. But either way, I'm not trying to portray myself as an expert. If you are an expert and you notice I'm doing something wrong, I'd love to hear from you!

# Stating the Problem

So, what I want to do here is get point estimates for the coefficients in a logistic regression model "by hand" (or mostly by hand). I'm going to be doing this in Julia, because I'm also interested in getting better at Julia stuff, but obviously the concepts are the same across any programming language.

# Setup

First, we'll load the libraries we're using here:

```{julia}
using GLM #to check my work against
using Distributions #for the Bernoulli distributions
using Random #to set a seed
using Optim #to do the acutal optimizing
using StatsFuns #for the logistic function, although this isn't strictly necessary since we'll write our own

Random.seed!(0408)
```

# Generate Data

Next, we'll generate some data to use to fit our logistic regression model. We'll start by generating our predictor matrix, **X**, and our $\beta$ values.

```{julia}
N = 100 #set our number of observations

#create a matrix with a column of 1s and 3 columns of normally distributed values.
X = hcat(ones(N), randn(N, 3)) 

#create some beta values to help generate y values
β = rand(Uniform(0, .5), 4)
```

Next, we'll write a logistic function that we can use to generate y values, given **X** and $\beta$

```{julia}
my_logistic(x) = exp.(x*b) ./ (1 .+ exp.(x*b))
```

One thing to keep in mind (and that always gets me!) is that, unlike R, Julia doesn't vectorize most operations by default, so you need to "broadcast" them using .  We can see this above like `.+`, which you can think of as like little compact for loops.

I learned that the above is how you take a real-valued number and transform it so that it's in the range [0, 1]. I wrote it out by hand, because I think it helps me remember/learn, but you could equivalently use the `logistic()` function in the StatsFuns package:

```{julia}
#| eval: false
logistic.(X*β)
```

And now we can generate our y values:

```{julia}
#y = Integer.(round.(my_logistic(X, β)))
y = Integer.(round.(logistic.(X*β)))
```

Since the outcome of `my_logistic()` will be a decimal, and logistic regression is used to model successes and failures, we want y to be either 0 or 1, not some number in between. So we'll need to round the output of `my_logistic()` to satisfy this assumption. We'll also need to cast it to be an integer.

# Define a Maximum Likelihood Estimator

Now that we have some data, we can write a function that uses maximum likelihood estimation to give us the best $\beta$ parameters for our given **X** and y. If you want to brush up on maximum likelihood, you can read my previous "learning out loud" post (INSERT LINK), or you can probably find materials written by someone who knows way more than I do. Either way, I'm not going to recap what MLE is here.

Let's define our function that we'll use to estimate $\beta$

```{julia}
function ml_logreg(x, y, b)

    #ŷ = my_logistic(x*b)
    ŷ = logistic.(x * b)

    res = Float64[]

    # for i in 1:lastindex(ŷ)
    #     push!(res, logpdf.(Bernoulli(ŷ[i]), y[i]))
    # end

    # ret = -sum(res)
    #all of the above is the same as:
    ret = sum(-1 .* (log.(ŷ) .* y .+ (log.(1 .- ŷ) .* (1 .- y))))
    return ret
end
```

So what's going on in this code?

1. We're getting $ŷ$ estimates for a given x and b by running them through the `my_logistic()` function. This will give us a 100x1 vector
1. We're instantiating an empty vector that will (eventually) contain Float64 values.
1. For each index in $ŷ$ (i.e. 1 through 100), we're getting the log-likelihood of the true outcome (y[i]) given a Bernoulli distribution parameterized by success rate $ŷ$[i].

I think this is the trickiest part of the whole problem, so I want to put it into words to make sure I understand it. In our problem, our y values are either 0 or 1. And the output of the `my_logistic()` function is going to be, for each y, a predicted probability that $y = 1$, i.e. a predicted success rate. Since a Bernoulli distribution is parameterized by a given success rate and models the outcome of a single yes/no (1/0) trial, it makes sense to use this to generate the likelihoods we want to maximize.

More concretely, the likelihoods we get will be dependent on:

1. the provided success rate *p*, and
1. the actual outcome

Where values of *p* that are closer to the actual outcome will be larger:

```{julia}
logpdf(Bernoulli(.5), 1)
```

```{julia}
#will be larger than the previous
logpdf(Bernoulli(.8), 1)
```

```{julia}
#will be even larger
logpdf(Bernoulli(.99), 1)
```

And inversely, you can imagine that if the outcome were 0, we'd want our predicted success rate to be very low.

Returning to our `ml_logreg()` function, what we're doing then is applying this logic to all of our $ŷ$ and corresponding *y* values (i.e. we're getting the likelihood of *y* for a given ŷ), and then we're creating a vector with all of these likelihoods -- that's what the `push!(...)` notation is doing -- pushing these likelihoods to the empty Float vector we created.

Finally, we're summing all of our likelihoods and then multiplying the result by negative one, since the optimizer we're using actually wants to *minimize* a loss function rather than *maximize* a loss function.

We can run this function by providing any X, y, and $\beta$, and it'll give us back a negative loglikelihood -- the negative sum of all of the individual likelihoods.

```{julia}
ml_logreg(X, y, β)
```

# Optimize $\beta$

So the above gives us the likelihood for a starting value of $\beta$, but we want to find the *best* values of $\beta$. To do that, we can optimize the function

```{julia}
res = optimize(params -> ml_logreg(X, y, params), β)
```

```{julia}
Optim.minimizer(res)
```

```{julia}
logreg_res = glm(X, y, Binomial())
```